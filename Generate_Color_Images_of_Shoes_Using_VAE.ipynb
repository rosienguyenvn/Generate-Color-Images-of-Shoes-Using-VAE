{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo5PziEC4hWs"
      },
      "source": [
        "# Generate Color Images of Shoes Using Variational Autoencoder (VAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2mLArkRSOgP"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj2oSb5sSRDr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, Conv2DTranspose, Input, Flatten, BatchNormalization, Lambda, Reshape, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import selu\n",
        "from keras.layers import Multiply, Add\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ajP_u73s6m"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4RrVVoTRHeX"
      },
      "outputs": [],
      "source": [
        "image_dir = \"shoes_trainB/\"\n",
        "images = [os.path.join(image_dir, image) for image in os.listdir(image_dir)]\n",
        "images[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2XC_W8ZabWu"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QTd_qpPabWu"
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "image_size = 64\n",
        "\n",
        "def preprocess(image):\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, (image_size, image_size))\n",
        "    image = image / 255.0\n",
        "    image = tf.reshape(image, shape = (image_size, image_size, 3,))\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfoBIhoTabWv"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((images))\n",
        "training_dataset = training_dataset.map(preprocess)\n",
        "training_dataset = training_dataset.shuffle(1000).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugl35xfdabWv"
      },
      "outputs": [],
      "source": [
        "len(training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Visualize the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "outputs": [],
      "source": [
        "# visualize some of them\n",
        "fig, axes = plt.subplots(5,5, figsize = (14,14))\n",
        "sample = training_dataset.unbatch().take(25)\n",
        "sample = [image for image in sample]\n",
        "\n",
        "idx = 0\n",
        "for row in range(5):\n",
        "    for column in range(5):\n",
        "        axes[row, column].imshow(sample[idx])\n",
        "        idx+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjJ6TPirabWx"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyUZi9J7abWx"
      },
      "source": [
        "### Build the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_U1ScatabWy"
      },
      "outputs": [],
      "source": [
        "latent_dim = 512\n",
        "encoder_input = Input(shape = (64,64,3))\n",
        "\n",
        "x = Conv2D(32, kernel_size=5, activation = LeakyReLU(0.02), strides = 1, padding = 'same')(encoder_input)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "filter_size = [64,128,256,512]\n",
        "for i in filter_size:\n",
        "    x = Conv2D(i, kernel_size=5, activation = LeakyReLU(0.02), strides = 2, padding = 'same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation = selu)(x)\n",
        "encoder_output = BatchNormalization()(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yng4V2NabWy"
      },
      "source": [
        "## Build the sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qacCqos4abWz"
      },
      "outputs": [],
      "source": [
        "# sampling layer (bottleneck)\n",
        "mu = Dense(latent_dim)(encoder_output)\n",
        "log_var = Dense(latent_dim)(encoder_output)\n",
        "\n",
        "epsilon = K.random_normal(shape = (tf.shape(mu)[0], tf.shape(mu)[1]))\n",
        "sigma = tf.exp(0.5 * log_var)\n",
        "\n",
        "z_eps = Multiply()([sigma, epsilon])\n",
        "z = Add()([mu, z_eps])\n",
        "\n",
        "encoder = Model(encoder_input, outputs = [mu, log_var, z], name = 'encoder')\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLhQMIntabW0"
      },
      "source": [
        "### Build the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_19TemjpabW0"
      },
      "outputs": [],
      "source": [
        "decoder = Sequential()\n",
        "decoder.add(Dense(1024, activation = selu, input_shape = (latent_dim, )))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.add(Dense(8192, activation = selu))\n",
        "decoder.add(Reshape((4,4,512)))\n",
        "\n",
        "decoder.add(Conv2DTranspose(256, (5,5), activation = LeakyReLU(0.02), strides = 2, padding = 'same'))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.add(Conv2DTranspose(128, (5,5), activation = LeakyReLU(0.02), strides = 2, padding = 'same'))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.add(Conv2DTranspose(64, (5,5), activation = LeakyReLU(0.02), strides = 2, padding = 'same'))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.add(Conv2DTranspose(32, (5,5), activation = LeakyReLU(0.02), strides = 2, padding = 'same'))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.add(Conv2DTranspose(3, (5,5), activation = \"sigmoid\", strides = 1, padding = 'same'))\n",
        "decoder.add(BatchNormalization())\n",
        "\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K822ux9QabW2"
      },
      "source": [
        "### Loss function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACu6DuzhabW2"
      },
      "outputs": [],
      "source": [
        "# vae loss = reconstruction loss + KL div\n",
        "\n",
        "def reconstruction_loss(y, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y - y_pred))\n",
        "\n",
        "def kl_loss(mu, log_var):\n",
        "    loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
        "    return loss\n",
        "\n",
        "def vae_loss(y_true, y_pred, mu, log_var):\n",
        "    return reconstruction_loss(y_true, y_pred) + (1 / (64*64)) * kl_loss(mu, log_var)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XrOzHSiabW3"
      },
      "source": [
        "### Conbine encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIse3VmhabW3"
      },
      "outputs": [],
      "source": [
        "mu, log_var, z = encoder(encoder_input)\n",
        "reconstructed = decoder(z)\n",
        "model = Model(encoder_input, reconstructed, name =\"vae\")\n",
        "loss = kl_loss(mu, log_var)\n",
        "model.add_loss(loss)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShvBJEARabW4"
      },
      "source": [
        "## Build function to save images while learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5tJNGgRabW4"
      },
      "outputs": [],
      "source": [
        "# make a function to save images while learning\n",
        "def save_images(model, epoch, step, input_):\n",
        "    prediction = model.predict(input_)\n",
        "    fig, axes = plt.subplots(5,5, figsize = (14,14))\n",
        "    idx = 0\n",
        "    for row in range(5):\n",
        "        for column in range(5):\n",
        "            image = prediction[idx] * 255\n",
        "            image = image.astype(\"int32\")\n",
        "            axes[row, column].imshow(image)\n",
        "            axes[row, column].axis(\"off\")\n",
        "            idx+=1\n",
        "    output_path = \"output/\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path)\n",
        "    plt.savefig(output_path + \"Epoch_{:04d}_step_{:04d}.jpg\".format(epoch, step))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bTeTKgeabW4"
      },
      "source": [
        "## Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym54InBSabW5"
      },
      "outputs": [],
      "source": [
        "random_vector = tf.random.normal(shape = (25, latent_dim,))\n",
        "save_images(decoder, 0, 0, random_vector)\n",
        "\n",
        "mse_losses = []\n",
        "kl_losses = []\n",
        "\n",
        "optimizer = Adam(0.0001, 0.5)\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    for step, training_batch in enumerate(training_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed = model(training_batch)\n",
        "            y_true = tf.reshape(training_batch, shape = [-1])\n",
        "            y_pred = tf.reshape(reconstructed, shape = [-1])\n",
        "            \n",
        "            mse_loss = reconstruction_loss(y_true, y_pred)\n",
        "            mse_losses.append(mse_loss.numpy())\n",
        "            \n",
        "            kl = sum(model.losses)\n",
        "            kl_losses.append(kl.numpy())\n",
        "            \n",
        "            train_loss = 0.01 * kl + mse_loss\n",
        "            \n",
        "            grads = tape.gradient(train_loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            \n",
        "            if step % 10 == 0:\n",
        "                save_images(decoder, epoch, step, random_vector)\n",
        "            print(\"Epoch: %s - Step: %s - MSE loss: %s - KL loss: %s\" % (epoch, step, mse_loss.numpy(), kl.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ocA2kjeabW5"
      },
      "source": [
        "## Visualize output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWGMPv9_abW5"
      },
      "source": [
        "### Display a generated image from the last training epoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mHZRJHQabW5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "plt.imshow(PIL.Image.open('output/Epoch_0068_step_0010.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCaAcuuXabW6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generate Color Images of Shoes Using VAE",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}